#  ollama create dolphin3.0-r1-mistral-24b:q6_k_l -f modelfiles/Modelfile-chatml

FROM ../Dolphin3.0-R1-Mistral-24B-Q6_K_L.gguf

SYSTEM """You are Dolphin, a helpful assistant with especially strong colding and reasoning skills. Ensure you complete the users requests in full."""

TEMPLATE """
{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ .Response }}<|im_end|>
"""


#  TEMPLATE """{{- range $i, $_ := .Messages }}
#  {{- $last := eq (len (slice $.Messages $i)) 1 -}}
#  <|im_start|>{{ .Role }}
#  {{ .Content }}{{ if not $last }}<|im_end|>
#  {{ end }}
#  {{- if and (ne .Role "assistant") $last }}<|im_end|>
#  <|im_start|>assistant
#  {{ end }}
#  {{- end }}"""


PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"

### Tuning ##
#  lowering batch to 256 to fit IQ4_XS with a decent num_ctx
#  PARAMETER num_batch 256
#  PARAMETER num_batch 1024

## For codegen ##
# 10240 is as high as I can go with 72B-IQ3_XXS and batch 512

PARAMETER num_ctx 32768

#  PARAMETER num_keep 512
#  PARAMETER temperature 0.2
#  PARAMETER top_p 0.6
#  PARAMETER presence_penalty 0.2
#  PARAMETER frequency_penalty 0.2
#  PARAMETER repeat_last_n 50

### min_p sampling ##
# min_p works best with a bit of temperature
PARAMETER temperature 0.2
# 1.0 disables top_p, so we can use min_p
PARAMETER top_p 1.0
PARAMETER min_p 0.9
### min_p sampling ##

#  SYSTEM """
#  You are an expert software engineer proficient in multiple programming languages. Your task is to generate, complete, and refactor code snippets based on the given instructions. Provide clean, efficient, and well-commented code.

#  Remember:
#  - Follow best practices and the latest conventions.
#  - Use the most current libraries, functions, and best practices. Avoid deprecated methods and outdated conventions.
#  - Ensure the code is easy to understand and maintain.
#  - Prefer British English spelling.
#  - Output the filename and line number when providing code snippets if you know them.
#  - When updating functions output the entire updated function.
#  - Think carefully about what the user is requesting and the provided context of our chat and code.

#  Avoid explanations unless explicitly prompted. Be concise and focus on the task.
#  """
